import torch.nn as nn
from .multihead_gate import MultiHeadAttentionGate

class ConvBlock(nn.Module):
    def __init__(self, c_in, c_out):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(c_in, c_out, 3, padding=1), nn.BatchNorm2d(c_out), nn.ReLU(inplace=True),
            nn.Conv2d(c_out, c_out, 3, padding=1), nn.BatchNorm2d(c_out), nn.ReLU(inplace=True),
        )
    def forward(self, x): return self.net(x)

class AttentionUNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, base_ch=32, num_heads=1):
        super().__init__()
        c=base_ch
        self.enc1=ConvBlock(in_channels, c)
        self.enc2=ConvBlock(c, 2*c)
        self.enc3=ConvBlock(2*c, 4*c)
        self.enc4=ConvBlock(4*c, 8*c)
        self.pool=nn.MaxPool2d(2)
        self.bottleneck=ConvBlock(8*c, 16*c)

        self.up4=nn.ConvTranspose2d(16*c, 8*c, 2, stride=2)
        self.g4 = MultiHeadAttentionGate(8*c, 8*c, 4*c, num_heads=max(1,num_heads))
        self.dec4=ConvBlock(16*c, 8*c)

        self.up3=nn.ConvTranspose2d(8*c, 4*c, 2, stride=2)
        self.g3 = MultiHeadAttentionGate(4*c, 4*c, 2*c, num_heads=max(1,num_heads))
        self.dec3=ConvBlock(8*c, 4*c)

        self.up2=nn.ConvTranspose2d(4*c, 2*c, 2, stride=2)
        self.g2 = MultiHeadAttentionGate(2*c, 2*c, c, num_heads=max(1,num_heads))
        self.dec2=ConvBlock(4*c, 2*c)

        self.up1=nn.ConvTranspose2d(2*c, c, 2, stride=2)
        self.dec1=ConvBlock(2*c, c)

        self.out=nn.Conv2d(c, out_channels, 1)

    def forward(self, x):
        e1=self.enc1(x); e2=self.enc2(self.pool(e1)); e3=self.enc3(self.pool(e2)); e4=self.enc4(self.pool(e3))
        b=self.bottleneck(self.pool(e4))
        d4=self.up4(b); e4g,_,_=self.g4(e4,d4); d4=self.dec4(nn.functional.pad(d4, (0,0,0,0)))
        d4=self.dec4(nn.functional.relu(nn.functional.conv2d(nn.functional.pad(nn.functional.relu(d4), (0,0,0,0)), self.dec4.net[0].weight, bias=self.dec4.net[0].bias, padding=0)))  # placeholder to ensure forward compiles; will refactor later
        # NOTE: keep as skeleton; youâ€™ll replace with proper cat([d4, e4g], dim=1) + dec4 etc.
        return self.out(d4)
